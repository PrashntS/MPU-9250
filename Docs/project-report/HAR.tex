
\documentclass[
10pt,           % Main document font size
a4paper,        % Paper type, use 'letterpaper' for US Letter paper
oneside,        % One page layout (no page indentation)
%twoside,       % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
%BCOR5mm,       % Binding correction
]{scrartcl}

\input{structure.tex}

\input{glossaries.tex}

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points

\title{\normalfont\spacedallcaps{Assistive Device to map and interpret the local scene for imparting Spatial Awareness in Visually Impaired People}}

\author{\spacedlowsmallcaps{Abhijeet Parmar* \& Prashant Sinha\textsuperscript{1}}}

\date{} % An optional date to appear under the author(s)

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\setlength{\voffset}{0cm}
\setlength{\hoffset}{0cm}

\includepdf[pages=-]{cover}

\newpage

\includepdf[pages=-]{cover_two}

\newpage

\setcounter{tocdepth}{3} % Set the depth of the table of contents

\tableofcontents    % Print the table of contents

\listoffigures      % Print the list of figures

%\listoftables       % Print the list of tables

\printglossary[style=long, title={List of Abbreviations}]

%: Abstract

\newpage

\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}
\markboth{Abstract}{}

Providing accurate and opportune information on people’s activities and behaviors is one of the most important tasks in pervasive computing. Innumerable applications can be visualized, for instance, in medical, security, entertainment, and tactical scenarios. In this study, we explore the detection of human activities (walk, run, jog, climb stairs up, climb stairs down, stand still, biking) from data acquired using single tri-axial accelerometer. We address the task of detecting activities irrespective of the user specific sensor position, orientation and body attachment. We compare the classification accuracies in case of orientation uncorrected data in different positions and orientations across multiple individuals. Also, we show that a single device and accelerometer data is sufficient to detect the activities. Additionally we also show significant activity detection accuracies when the user holds the device in the hand or carries it in the pockets, as in a real life scenario. Random Forest classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of $96\%$. Finally, we present some open problems and ideas that, due to their high relevance, should be addressed in future research.

\section*{Keywords}
\textit{Human Activity Recognition, Feature Extraction, Windowing, Pervasive Computing, Random Forest}

\newpage

\section*{Acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}
\markboth{Acknowledgements}{}

\setlength{\parindent}{0em}
\setlength{\parskip}{0.5em}

\emph{Thank You.}

\emph{My sincerest gratitude for your support, patience, guidance:}

\begin{itemize}
  \item Ir. Abhijeet Parmar, Academic Supervisor, DIC, for his continuous support and guidance of my Summer Internship Project, for his scientific intuition which made him a constant source of ideas and inspirations.
  \item Prof. Shobha Bagai, Program Coordinator (B.Tech. ITMI), CIC, for her advice, supervision and immense knowledge, for her critical comments and suggestions for improving my work.
  \item Prof. B. Biswal, Coordinator, DIC, for providing excellent research environment and infrastructure to carry forward my research.
  \item All CIC faculty, staff and students who transferred their immense knowledge and skill for exceptionally enriching my intellectual maturity.
  \item Prof. Alex "Sandy" Pentland, MIT Media Lab, for his willingness to critically review my research method, result and answering my questions about human activity behaviours.
  \item Prof. Samir K. Brahmachari, Founder Director, CSIR-Institute of Genomics and Integrative Biology, Chief Mentor, CSIR-OSDD Unit, for his qualified remarks which helped me in refining my model and results.
\end{itemize}

\emph{For your time and cooperation:}

\begin{itemize}
  \item Pragya Jaiswal, and Parul Sethi for their precious time for assisting in data collection task.
  \item Devesh Khandelwal for his insights in programming methods.
\end{itemize}

\emph{To my friends, old, new and future throughout the globe.}

\emph{Above all, for your love and support:}

My Family.

\
\

Sincerely,

(Prashant Sinha)

%: Introduction

\setlength{\parindent}{1em}

\newpage
\section{Introduction}

\subsection{Statement of the Problem}
Visually impaired people confront a number of challenges in their daily life - from figuring out if they are at the correct bus stop to reading the label on their soft drink bottle. Navigation, for instance, is very difficult for a person and prevents the user from moving independently or walk safely.

A number of assistive technologies are available for users now a days which help a visually impaired person to perform their day to day activities, such as:

\begin{enumerate}
	\item Screen Reader: Allows the user to use a computer and mobile phone. It is an example of software based assistance that reads the content of the screen to the user and allows them to use the device as a regular person. It is not perfect, for, it requires the developers of the apps to incorporate the assistive guidelines during the development process.
	\item GPS and Navigation: Allows the users to pinpoint their location on map, and also get step by step directions to a specified locations. This helps users to navigate and prevents them from losing their way.
	\item Refreshable Braille Display: It consists of electronically actuated dots in a matrix of 4x2 dots per character. The dots raise and depress to form each of the character, therefore making use of braille script to present the characters to the user.
	\item Tactile paving on sidewalks, metro stations and shopping complexes; elevators with braille embossed buttons; and announcements in trains also aid into facilitating the visually impaired people.
\end{enumerate}

However, it is still very difficult to navigate and identify objects in new or strange locations, or locations that are not designed keeping accessibility in mind. They’ll also require constant assistance from other people, which is, in turn, subject to knowledge and behaviour of other people and has its own social, and mental consequences.

The navigation technologies that are available today can indeed give navigation instructions from a location to another, however, the instructions require personal judgement of the user. These include avoiding hitting various obstacles, staying on side walk, and so on. For a blind person, following these instructions is not less than a challenge either.

Hence, it is required that proper steps be taken to make a visually impaired person partially or fully self reliable and hence support them to live independently.

\subsection{Proposed Solution}
Through the global penetration of Smart Phones, access to Navigation Services that are assisted though GPS are available very easily and are cost affective. However, the instructions provided by the Navigation Services require personal judgement of the user to follow. This includes avoiding obstacles that are encountered on the way, staying on the sidewalk and in correct direction and so on. This is not an easy task for a visually impaired person. The services also fail to work in an indoor environment.

Hence, our solution proposes that increasing the local spatial awareness is a key solution to this problem, and would help the visually impaired people gain more self-reliance.

\subsection{Proposed Work}

\begin{enumerate}
  \item Building a wearable assistive device
  \item Motion context awareness using motion trackers
  \item Scene description and object detection using 3D point cloud
  \item Haptic and auditory interface for interaction
\end{enumerate}

%: Motion Tracking

\newpage
\section{Motion Tracking using Inertial Measurement Unit}

\subsection{Background}
Motion tracking refers to the process of recording the movement of objects. Various passive and active methods may be utilized for achieving this purpose, such as video surveillance, low range RADAR or motion sensors. An Inertial Measurement Sensor is an array of electronic sensors that are used to measure the external forces acting on an object. An IMU may contain an Accelerometer, Gyroscope, Magnetometer, GPS, Pressure Sensor etc.

\begin{enumerate}
	\item An accelerometer measures the acceleration in a axis.
	\item A gyroscope measures the rate of rotation in a particular axis.
	\item A magnetometer (also known as a digital compass) measures the magnitude of magnetic field in a particular axis.
\end{enumerate}

Through a combination of these three devices, in the three orthogonal axes, it is possible to accurately reproduce the motion of an object the \gls{IMU} is pivoted to.

\subsection{Motion Tracking Device}
To track and record the motion data, we built a shield for an Arduino Uno (Figure \ref{tracker_shield}). The motion tracking shield uses a tri-axial \gls{IMU} (Figure \ref{Axes_Config}) having an Accelerometer, a Gyroscope and a Magnetometer for motion tracking. The shield also has a 16 GB onboard flash storage to record the IMU data samples. The schematic for the shield is available in Appendix.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{Arduino_Tracker}
  \end{center}
  \vspace{-10pt}
  \caption{\label{tracker_shield}Arduino Motion Tracker Shield prototype.}
  \vspace{-10pt}
\end{figure}

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{Axes_Config}
  \end{center}
  \vspace{-10pt}
  \caption{\label{Axes_Config}IMU Sensor Axes Configuration}
  \vspace{-10pt}
\end{figure}

\subsubsection{Sensor data fusion}
Data from the three axes of the accelerometer, gyroscope and magnetometer are fused to get the attitude and heading information of the device. The information are given by quaternion angles as Yaw, Pitch and Roll. A Quaternion filter was implemented following the method proposed by \cite{Madgwick2011}.

\subsubsection{Data logging and Visualisation}
We used InfluxDB (Figure \ref{Annotation_UI}) for efficient recording and retrieval of time series sensor data. Multiple interface for data logging was provided, which includes UDP channel over WiFi, USB Serial, and an on-board MicroSD card.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\textwidth]{Annotation_UI}
  \end{center}
  \vspace{-10pt}
  \caption{\label{Annotation_UI}Annotation and Visualisation dashboard.}
  \vspace{-10pt}
\end{figure}

%: HAR

\newpage
\section{Realtime Human Activity Recognition Task}

\subsection{Background}

Providing and predicting information about user's intent, behaviour, and activities is one of the most important task in pervasive computing. Human Activity Recognition (\gls{HAR}) is a problem of inferring the state of the user by exploiting the data obtained from sensors that are already present in many mobile devices. The information provided from \gls{HAR} system can be utilised to provide contextual awareness which can further be utilised in innumerable applications. Context aware mobile apps may optimize their user experience based on the activity that the user is presently performing. It can also be used in various patient and baby monitoring systems to provide accurate and realtime activity data which may prove useful to the caretakers in case of emergencies \cite{Anguita2013}. Activity information may also be used to remove false positives from pedometer data in fitness tracking applications for more accurate gait analysis.

\subsection{Previous Work}

Various studies have shown efficiency of machine learning methods for the solution of \gls{HAR} task. \cite{Anguita2013} proposes an energy efficient method of activity classification using fixed point arithmetic scheme for training a \gls{SVM}. The classification accuracy of their scheme is 89\% for 6 activity classes. \cite{Gupta2014} is the closest to the present study. They take a 6 seconds window to compute the features. The method involves further windowing to calculate the features such as mean trends and windowed mean difference. Their result had an accuracy of 97-98\% using kNN and Naive Bayes classifier.

\cite{Banos2014} investigate the relation between window length and the classification accuracy, while \cite{Reyes-Ortiz2015} report success using signal processing and filters on the combined gyroscope and accelerometer data.

\cite{He2009} use PCA and Cosine transform for the \gls{HAR} task. They report 97\% accuracy between 4 activity classes. \cite{Shoaib2013} also use gyroscope and accelerometer data in their study while claiming the decrease in classification accuracy due to the bias arising by shifting the position of the device.

\subsection{Undertaken Challenge}
\subsubsection{Hardware Complexity Constraint}
It is required that the task is solved solely through the data from a single triaxial accelerometer. This reduces the hardware complexity.

\subsubsection{Attitude Independent Recognition (Axis Constraint)}
\label{axis_constraint}
It is required that the task be solved without any prior knowledge of the location and orientation of the motion tracker. This follows from the fact that the motion tracker could be placed at any spot on the user, for example their pockets, their arm or in their hand.

\subsubsection{Hardware Friendly Classification (Time Complexity Constraint)}
\label{time_constraint}
It is required that the features that are generated from the sensor sample sequence must be computationally low cost. This is required for energy efficient implementation of the technique in various portable devices such as a fitness tracker or a mobile phone.

\subsection{Method}

\begin{figure}[p]
  \begin{center}
    \includegraphics[width=\textwidth]{HAR_Illustration}
  \end{center}
  \vspace{-10pt}
  \caption{\label{har_illustration}Methodology Overview}
\end{figure}

The task of human activity recognition (HAR) begins with data collection. However, the raw data in itself is not representative enough to be used for carrying out the task of classification. Thus, we need to prepare the data by cleaning it and carrying out various data preprocessing activities. The preprocessed data still doesn’t capture the temporal nature of the task. Hence data segmentation is performed to enhance the discriminative power of the data. Feature extraction, then, converts the signals into the most relevant and powerful features which are unique for the activity at hand (Figure \ref{har_illustration}). Finally classification algorithms can be trained on the refined data, and a reliable prediction model can be created

\subsubsection{Acquisition of Activity Dataset}
Motion data was logged through motion tracking device and an iPhone app developed for the purpose at the sampling rate of 50 Hz while multiple human subjects performed various activities as mentioned in section \ref{activity_classes}. The location of the devices and the duration of activities were neither controlled nor taken into consideration. The subjects were also allowed to move or use the devices during the activity. These events were also not controlled and did not have any exclusive label. This was done to ensure that the dataset reflected the real-world usage pattern. The starting and ending timestamp of the activities were used to label the logged data.

We also obtained the public dataset \cite{Reyes-Ortiz2015} and \cite{Shoaib2014} to further increase the volume of our data. Only the accelerometer data was utilised from the obtained dataset.

\subsubsection{Activity Classes}
\label{activity_classes}
We define following activity classes to set the scope of the HAR task.
\begin{enumerate}
	\item Stationary
	\begin{enumerate}
		\item Sitting
		\item Standing
	\end{enumerate}
	\item Active
	\begin{enumerate}
		\item Walking
		\begin{enumerate}
			\item Typical
			\item Stair
			\begin{enumerate}
				\item Walking Upstairs
				\item Walking Downstairs
			\end{enumerate}
		\end{enumerate}
		\item Running
		\begin{enumerate}
			\item Typical
			\item Jogging
		\end{enumerate}
		\item Biking
	\end{enumerate}
\end{enumerate}

\subsubsection{Windowing and Overlapping}
Due to the nature of task, it is often very difficult to represent a particular activity solely through one sample point. Hence, it is recommended to utilise a sequence of signal samples to represent the activity instances \cite{Banos2014}.

In the obtained discrete-time sample sequence, we defined a window of length having $l$ samples for all the three axes as activity instance. The windows were also overlapped to reduce any segmentation bias. The value of $l$, and its effect on classification accuracy is discussed in section \ref{results}.

\subsubsection{Feature Engineering}
To reduce the computational complexity and enhance the recognition characteristics, feature generation is required. Overall 5 features were calculated for each axis. To conform to the axis constraints as defined in section \ref{axis_constraint}, the features from the three axes were combined to get a final feature vector.

To address the time complexity constraint as defined in section \ref{time_constraint}, all the features are calculated in time domain, since the frequency domain conversion using discrete Fourier transform (\gls{DFT}) is a computationally intensive process.

From the figure \ref{example_activity_instance} it can be observed that various activities are inherently periodic in nature. The features are calculated for all the three axes of the activity instances obtained after windowing and overlapping the data.

\begin{figure}[b]
  \begin{center}
    \includegraphics[width=\textwidth]{example_activity_instance}
  \end{center}
  \vspace{-10pt}
  \caption{\label{example_activity_instance}Example instance frames for various activities}
\end{figure}

\subsubsection{Axial Features}
\label{axial_feature}
Mathematically, the activity instance can be represented by a subsequence of discrete acceleration sample sequence.

Let $S_{raw}$ be a sequence of ordered discrete-time acceleration sample sequence as obtained from the motion tracker. Here, 

\begin{equation} \label{eq:s_raw}
S_{raw}=(\langle a^x_t,a^y_t,a^z_t \rangle)
\end{equation}

where $a^x_t$, $a^y_t$ and $a^z_t$ represent the accelerometer reading at instance $t$, in $x$, $y$ and $z$ axis respectively.

We define the activity instance $S_{act}$ as the subsequence of $S_{raw}$ having length $l$. From $S_{act}$, we further define decomposed axial discrete-time sample sequence by equation \ref{eq:skact}, and indexed sequence by \ref{eq:skrmp}.

\begin{equation} \label{eq:skact}
S^k_{act}=(a^k_i)
\end{equation}


\begin{equation} \label{eq:skrmp}
S^k_{rmp}=(\langle i, a^k_i \rangle)
\end{equation}


where $k\in\langle x, y, z \rangle $ and $i$ is the sequence index.

The axial feature vector is defined in the equation \ref{eq:fk}.

\begin{equation} \label{eq:fk}
F^k = \langle a_{auc}^k, a_{tss}^k, a_{kp}^k, a_{binnedkp}^k, a_{sm}^k \rangle
\end{equation}


The feature vector components are discussed below.

\paragraph{Absolute area under linearly interpolated curve}

Let $f^l_{[i,j]}$ denote the two-point form of a straight line segment joining arbitrary elements in $S^k_{rmp}$. Here,

\begin{equation} \label{eq:fl_ij}
f^l_{[i,j]}=(\frac{a^k_j-a^k_i}{j-i}(x-i)+a^k_j) \quad x \in [i, j)
\end{equation}


We may obtain the linearly interpolated curve function $f_{int}$ as a piecewise function of multiple line segments \cite{Bradie} , defined by$f^l_{[i,j]}$. Hence, if $l$ denotes the number of elements in sequence $S^k_{rmp}$, we get equation \ref{eq:fintk}.

\begin{equation} \label{eq:fintk}
f_{int}^k = (f^l_{[i,i+1]})_{i=0}^{l-1}
\end{equation}


We take the absolute area under $f_{int}^k$ as a feature, which is given by equation \ref{eq:a_auc}

\begin{equation} \label{eq:a_auc}
a_{auc}^k=\int_0^l |f_{int}^k| dx
\end{equation}


\paragraph{Total sum of squares}
Let $\bar{a}^k$ denote the mean of $S^k_{act}$. Then, we obtain the total sum of square for series $S^k_{act}$ as:

\begin{equation} \label{eq:a_tss}
a_{tss}^k = \sum_{n=0}^{l}(a^k_n - \bar{a}^k)^2 \quad \forall a^k_n \in S^k_{act}
\end{equation}

The quantity obtained from equation \ref{eq:a_tss} is used as a feature.

\paragraph{Key point}

We define the key points, $S^k_{kp}$, a subsequence of $S^k_{rmp}$, where the signal sequence attains relative maximum or minimum value (extrema values).

Due to noisy nature of the signal, the relative extrema are estimated by taking $n$ neighbours of the element currently in consideration. The quantity $n$ is defined as the 'order' of extrema function. This is done to ensure that small fluctuations are not accounted as key points.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{keypoint_feature}
  \end{center}
  \vspace{-10pt}
  \caption{\label{keypoint_feature}Sample key points selected from raw signal.}
  \vspace{-10pt}
\end{figure}

\paragraph{Key point gradient}
Let

\begin{equation} \label{eq:gk_kp}
G^k_{kp} = (\frac{a^k_j-a^k_i}{j-i})_{i,j}
\end{equation}


be a sequence defined from all consecutive elements $i$ and $j$ in key point sequence $S^k_{kp}$. Here, the elements of sequence $G^k_{kp}$ denote the gradient of a straight line joining the key points.

We take the quantity from equation \ref{eq:a_kp} as a feature.

\begin{equation} \label{eq:a_kp}
a_{kp}^k = Var[G^k_{kp}]
\end{equation}


\paragraph{Binned key point gradient}
We define gradient binning as the process of grouping straight lines on the basis of their gradient. The grouping ranges are obtained by dividing the I and IV quadrants (within the domain of $\arctan$) of cartesian plane in, say, $h$ equal segments. The binning ranges are defined by the following sequence:

\begin{equation} \label{eq:gbin}
G_{bin} = (\langle i, \intco{\tan(\frac{\pi}{h}i), \tan(\frac{\pi}{h}(i+1))} \rangle)_{i=\lceil\frac{h}{2}\rceil}^{\lfloor\frac{h}{2}\rfloor} \quad i \in \mathbb{Z}
\end{equation}


The binning function $f_{bin}$ follows from $G_{bin}$ where, the gradients are given index number $i$, if they fall in corresponding range.

Using $f_{bin}$, we define equation \ref{eq:g_k_binnedkp}, where, $a_i$ are the elements in $G^k_{kp}$.

\begin{equation} \label{eq:g_k_binnedkp}
G^k_{binned kp} = (f_{bin}(a_i))
\end{equation}


We take the following quantity as a feature:

\begin{equation} \label{eq:a_bin_kp}
a_{binnedkp}^k = Var[G^k_{binned kp}]
\end{equation}


\paragraph{Variance of smoothened series}
We use a simple rolling average on $S^k_{act}$ to smooth out the short-term fluctuations in the sequence. We form a new sequence $S^k_{smooth}$ having length $l-n$, where $l$ is the window length and $n$ is the order of rolling average, from $S^k_{act}$. For the elements $e$ in $S^k_{act}$, we have

\begin{equation} \label{eq:sk_smooth}
S^k_{smooth} = (\frac{e_{m}+e_{m-1}+...+e_{m-(n-1)}}{n})_{m=0}^{l-n}
\end{equation}

The following quantity is taken as a feature.
\begin{equation} \label{eq:sk_smooth}
a_{sm}^k = Var[S^k_{smooth}]
\end{equation}

\subsubsection{Axial Feature Pooling and Feature Vector}
To conform to the Axis Constraint, as defined in section \ref{axis_constraint}, the axial feature vectors $F^x$, $F^y$, and $F^z$ are combined (pooling) to obtain an axis independent feature vector. The variance features, $ \{a_{kp}^k, a_{binnedkp}^k, a_{sm}^k\} $ are pooled by taking the combined variance. We take arithmetic mean of feature $ \{a_{auc}^k, a_{tss}^k\} $. Here, $k$ denotes the three orthogonal axes $x$, $y$, and $z$ as mentioned in section \ref{axial_feature}.


\subsubsection{Classification using Supervised Learning}
Various supervised learning schemes were utilised for the motion classification. The classifiers were trained using the labelled feature vectors and the resulting classifier was then used for cross validation and prediction.

\subsubsection{Realtime Activity Prediction}
To implement the realtime activity classification, the triaxial accelerometer is sampled at a sampling rate of 50 Hz. These samples are stored in a buffer array and then the feature vector is  calculated for this buffer array. The predicted activity label for this feature vector is obtained by fitting the vector into the trained classifiers.

\subsection{Feature Classification}
\label{feature_classification}

Since the \gls{HAR} task requires that multiple activities are classified, a multi-class classifier is required. We explored various multi-class supervised-learning methods to evaluate the classification performance of the features as discussed in section \ref{axial_feature}. The methods that were evaluated are: Decision Tree Classifier (\gls{DTC}), Random Forest Classifier (\gls{RFC}), and Support Vector Machine (\gls{SVM}) with \gls{RBF} Kernel with varying parameters.

The performance as well as the relevance of the features are discussed in section \ref{results}.

\subsection{Results and Discussions}
\label{results}
\subsubsection{Results}

After calculating the features for the windows, and assigning the labels, we split the dataset into two segments with random cuts. First segment, having $75\%$ the size of the dataset, was used for training the classifiers. The other segment, representing the $25\%$ of the dataset, was used for testing purpose. The same data-subset was used for evaluating all the classifiers as mentioned in section \ref{feature_classification}.

Figure \ref{cls_rpt} contains the classification accuracy obtained by each classifier with respect to the chosen window length. Using the dataset as mentioned in section \ref{results}, we obtained a maximum classification accuracy of $96.32\%$ using Random Forest Classifier closely followed by Support Vector Machine with $95.05\%$ accuracy (Figure \ref{cm_fig}).

\begin{figure}[h]
  \centering
  \begin{center}
    \includegraphics[width=\textwidth]{Classification_Report}
  \end{center}
  \vspace{-10pt}
  \caption{\label{cls_rpt}Comparative classification accuracy for varying window sizes.}
  \vspace{10pt}
\end{figure}

\begin{figure}[h]
  \centering
  \begin{center}
    \includegraphics[width=\textwidth]{Confusion_Matrix}
  \end{center}
  \vspace{-10pt}
  \caption{\label{cm_fig}Confusion Matrix for window length $l=100$.}
\end{figure}

\subsubsection{Discussions}
Human activities are performed during relatively long periods of time compared to the sensors sampling rate. Besides, a single sample on a specific time instant does not provide sufficient information to describe the performed activity. Thus, activities need to be recognized in a time window basis rather than in a sample basis. Now, the question is: how do we compare two given time windows? It would be nearly impossible for the signals to be exactly identical, even if they come from the same subject performing the same activity. This is the main motivation for applying feature extraction methodologies \cite{Guyon2003} \cite{Chen2006} to each time window: filtering relevant information and obtaining quantitative measures that allow signals to be compared.In general, two approaches have been proposed to extract features from time series data: statistical and structural. In our study, we have used a combination of both statistical and structural features.

In addition to efficient feature extraction, selection of window length that is dividing the measured time series in time windows is also crucial in increasing the classification accuracy. A key factor is, therefore, the selection of the window length because the computational complexity of any feature extraction method depends on the number of samples. Having rather short windows may enhance the feature extraction performance, but would entail higher overhead due to the recognition algorithm being triggered more frequently. Besides, short time windows may not provide sufficient information to fully describe the performed activity. Conversely, if the windows are too long, there might be more than one activity within a single time window. Different window lengths have been used in the study but the maximum classification accuracy is observed with window length of 2s. Of course, this decision is conditioned to the activities to be recognized and the measured attributes.

Figure \ref{parallel_plot} is the parallel coordinate visualisation \cite{Wegman1990} of $50$ feature vectors, chosen randomly, for each activity class as described in section \ref{activity_classes}. We used parallel coordinates to plot the components in order as presented in figure. The independence between individual features is clearly distinguished for the activity classes. The feature vectors are plotted in both Linear and Logarithmic scale to improve visibility of features that attain value close to zero, since the features are not normalised.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=\textwidth]{ParallelPlot}
  \end{center}
  \vspace{-10pt}
  \caption{\label{parallel_plot}Parallel coordinates plot of feature vectors.}
  \vspace{5pt}
\end{figure}


Similarly, figure \ref{radviz_plot} is the RadViz Visualisation \cite{Sharko2008}\cite{Bertini2005} of 400 feature vectors chosen randomly for each activity classes. Distinct clusters of activity classes further support the conclusion made from the Parallel Coordinate plot of feature vector. Confusion between similar activities, such as Jogging and Running, and Walking and Climbing is also visible in the plot wherever the cluster of classes overlap. However, it should be noted here that only a small percentage of feature vectors overlap, as evident from section b in figure \ref{radviz_plot}.

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=\textwidth]{RadViz}
  \end{center}
  \vspace{-10pt}
  \caption{\label{radviz_plot}RadViz visualisation of feature vectors.}
  \vspace{-10pt}
\end{figure}


\newpage
\section{Conclusion}
The focus of this research was to explore and analyze flexibility in various constraints, specifically the position, orientation and attachment of the mobile sensor device in a human activity recognition task. The results from our study provide evidence to the fact that activity recognition is possible in spite of variation in the aforementioned constraints. Our findings also indicate that neither device orientation information nor orientation correction appear to be necessary, as we are able to correctly classify the activities across all subjects without orientation correction. Moreover, by using simple features, we can achieve high classification accuracies, above 95\% irrespective of how the device is carried and placed by the subject. This method is suitable for a distributed system where users can report data, in real-time or in batches, and this information can be processed and used to act according to the individual context.

The results of the present research have been submitted for publication in International Journal of Human-Computer Studies.

\section{Future Work}
\begin{itemize}
  \item 3D Point Cloud generation using Time of Flight sensor.
  \item Development of intelligent Scene Description methodologies.
  \item Final assistive navigation device prototyping.
  \item Testing on live subjects.
\end{itemize}

\newpage

\renewcommand{\refname}{\spacedlowsmallcaps{References}} % For modifying the bibliography heading
\addcontentsline{toc}{section}{References}
\markboth{References}{}
\bibliographystyle{ieeetr}

\bibliography{bibliography.bib} % The file containing the bibliography

\newpage

\section*{Appendix 1}
\addcontentsline{toc}{section}{Appendix}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[width=0.6\textwidth]{Autocorrelation}
  \end{center}
  \vspace{-10pt}
  \caption*{\label{keypoint_feature}Example autocorrelation plot.}
  \vspace{-10pt}
\end{figure}


\newpage

\markboth{Appendix}{Appendix 1}
\setlength{\voffset}{0cm}
\setlength{\hoffset}{0cm}

\includepdf[pages=-]{shield}

\setlength{\voffset}{-2.54cm}
\setlength{\hoffset}{-2.54cm}

\end{document}
